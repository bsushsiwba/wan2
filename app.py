import torch
from diffusers.pipelines.wan.pipeline_wan_i2v import WanImageToVideoPipeline
from diffusers.models.transformers.transformer_wan import WanTransformer3DModel
from diffusers.utils.export_utils import export_to_video
import gradio as gr
import tempfile
import numpy as np
from PIL import Image
import random
import gc

from torchao.quantization import quantize_
from torchao.quantization import Float8DynamicActivationFloat8WeightConfig
from torchao.quantization import Int8WeightOnlyConfig

import aoti


MODEL_ID = "Wan-AI/Wan2.2-I2V-A14B-Diffusers"

MAX_DIM = 832
MIN_DIM = 480
SQUARE_DIM = 640
MULTIPLE_OF = 16

MAX_SEED = np.iinfo(np.int32).max

FIXED_FPS = 16
MIN_FRAMES_MODEL = 8
MAX_FRAMES_MODEL = 80

MIN_DURATION = round(MIN_FRAMES_MODEL / FIXED_FPS, 1)
MAX_DURATION = round(MAX_FRAMES_MODEL / FIXED_FPS, 1)


pipe = WanImageToVideoPipeline.from_pretrained(
    MODEL_ID,
    transformer=WanTransformer3DModel.from_pretrained(
        "cbensimon/Wan2.2-I2V-A14B-bf16-Diffusers",
        subfolder="transformer",
        torch_dtype=torch.bfloat16,
        device_map="cuda",
    ),
    transformer_2=WanTransformer3DModel.from_pretrained(
        "cbensimon/Wan2.2-I2V-A14B-bf16-Diffusers",
        subfolder="transformer_2",
        torch_dtype=torch.bfloat16,
        device_map="cuda",
    ),
    torch_dtype=torch.bfloat16,
).to("cuda")

pipe.load_lora_weights(
    "Kijai/WanVideo_comfy",
    weight_name="Lightx2v/lightx2v_I2V_14B_480p_cfg_step_distill_rank128_bf16.safetensors",
    adapter_name="lightx2v",
)
kwargs_lora = {}
kwargs_lora["load_into_transformer_2"] = True
pipe.load_lora_weights(
    "Kijai/WanVideo_comfy",
    weight_name="Lightx2v/lightx2v_I2V_14B_480p_cfg_step_distill_rank128_bf16.safetensors",
    adapter_name="lightx2v_2",
    **kwargs_lora
)
pipe.set_adapters(["lightx2v", "lightx2v_2"], adapter_weights=[1.0, 1.0])
pipe.fuse_lora(adapter_names=["lightx2v"], lora_scale=3.0, components=["transformer"])
pipe.fuse_lora(
    adapter_names=["lightx2v_2"], lora_scale=1.0, components=["transformer_2"]
)
pipe.unload_lora_weights()

quantize_(pipe.text_encoder, Int8WeightOnlyConfig())
quantize_(pipe.transformer, Float8DynamicActivationFloat8WeightConfig())
quantize_(pipe.transformer_2, Float8DynamicActivationFloat8WeightConfig())

aoti.aoti_blocks_load(pipe.transformer, "zerogpu-aoti/Wan2", variant="fp8da")
aoti.aoti_blocks_load(pipe.transformer_2, "zerogpu-aoti/Wan2", variant="fp8da")


default_prompt_i2v = (
    "ì´ ì´ë¯¸ì§€ì— ìƒë™ê°ì„ ë¶€ì—¬í•˜ê³ , ì˜í™” ê°™ì€ ì›€ì§ì„ê³¼ ë¶€ë“œëŸ¬ìš´ ì• ë‹ˆë©”ì´ì…˜ì„ ì ìš©"
)
default_negative_prompt = "ìƒ‰ì¡° ì„ ëª…, ê³¼ë‹¤ ë…¸ì¶œ, ì •ì , ì„¸ë¶€ íë¦¼, ìë§‰, ìŠ¤íƒ€ì¼, ì‘í’ˆ, ê·¸ë¦¼, í™”ë©´, ì •ì§€, íšŒìƒ‰ì¡°, ìµœì•… í’ˆì§ˆ, ì €í’ˆì§ˆ, JPEG ì••ì¶•, ì¶”í•¨, ë¶ˆì™„ì „, ì¶”ê°€ ì†ê°€ë½, ì˜ëª» ê·¸ë ¤ì§„ ì†, ì˜ëª» ê·¸ë ¤ì§„ ì–¼êµ´, ê¸°í˜•, ë³€í˜•, í˜•íƒœ ë¶ˆëŸ‰ ì‚¬ì§€, ì†ê°€ë½ ìœµí•©, ì •ì§€ í™”ë©´, ì§€ì €ë¶„í•œ ë°°ê²½, ì„¸ ê°œì˜ ë‹¤ë¦¬, ë°°ê²½ ì‚¬ëŒ ë§ìŒ, ë’¤ë¡œ ê±·ê¸°"


def resize_image(image: Image.Image) -> Image.Image:
    width, height = image.size

    if width == height:
        return image.resize((SQUARE_DIM, SQUARE_DIM), Image.LANCZOS)

    aspect_ratio = width / height

    MAX_ASPECT_RATIO = MAX_DIM / MIN_DIM
    MIN_ASPECT_RATIO = MIN_DIM / MAX_DIM

    image_to_resize = image

    if aspect_ratio > MAX_ASPECT_RATIO:
        target_w, target_h = MAX_DIM, MIN_DIM
        crop_width = int(round(height * MAX_ASPECT_RATIO))
        left = (width - crop_width) // 2
        image_to_resize = image.crop((left, 0, left + crop_width, height))
    elif aspect_ratio < MIN_ASPECT_RATIO:
        target_w, target_h = MIN_DIM, MAX_DIM
        crop_height = int(round(width / MIN_ASPECT_RATIO))
        top = (height - crop_height) // 2
        image_to_resize = image.crop((0, top, width, top + crop_height))
    else:
        if width > height:
            target_w = MAX_DIM
            target_h = int(round(target_w / aspect_ratio))
        else:
            target_h = MAX_DIM
            target_w = int(round(target_h * aspect_ratio))

    final_w = round(target_w / MULTIPLE_OF) * MULTIPLE_OF
    final_h = round(target_h / MULTIPLE_OF) * MULTIPLE_OF

    final_w = max(MIN_DIM, min(MAX_DIM, final_w))
    final_h = max(MIN_DIM, min(MAX_DIM, final_h))

    return image_to_resize.resize((final_w, final_h), Image.LANCZOS)


def get_num_frames(duration_seconds: float):
    return 1 + int(
        np.clip(
            int(round(duration_seconds * FIXED_FPS)),
            MIN_FRAMES_MODEL,
            MAX_FRAMES_MODEL,
        )
    )


def get_duration(
    input_image,
    prompt,
    steps,
    negative_prompt,
    duration_seconds,
    guidance_scale,
    guidance_scale_2,
    seed,
    randomize_seed,
    progress,
):
    BASE_FRAMES_HEIGHT_WIDTH = 81 * 832 * 624
    BASE_STEP_DURATION = 15
    width, height = resize_image(input_image).size
    frames = get_num_frames(duration_seconds)
    factor = frames * width * height / BASE_FRAMES_HEIGHT_WIDTH
    step_duration = BASE_STEP_DURATION * factor**1.5
    return 10 + int(steps) * step_duration


def generate_video(
    input_image,
    prompt,
    steps=4,
    negative_prompt=default_negative_prompt,
    duration_seconds=MAX_DURATION,
    guidance_scale=1,
    guidance_scale_2=1,
    seed=42,
    randomize_seed=False,
    progress=gr.Progress(track_tqdm=True),
):
    if input_image is None:
        raise gr.Error("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

    num_frames = get_num_frames(duration_seconds)
    current_seed = random.randint(0, MAX_SEED) if randomize_seed else int(seed)
    resized_image = resize_image(input_image)

    output_frames_list = pipe(
        image=resized_image,
        prompt=prompt,
        negative_prompt=negative_prompt,
        height=resized_image.height,
        width=resized_image.width,
        num_frames=num_frames,
        guidance_scale=float(guidance_scale),
        guidance_scale_2=float(guidance_scale_2),
        num_inference_steps=int(steps),
        generator=torch.Generator(device="cuda").manual_seed(current_seed),
    ).frames[0]

    with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmpfile:
        video_path = tmpfile.name

    export_to_video(output_frames_list, video_path, fps=FIXED_FPS)

    return video_path, current_seed


# ì„¸ë ¨ëœ í•œê¸€ UI
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¬ WAN ê¸°ë°˜ ì´ˆê³ ì† ì´ë¯¸ì§€ to ë¹„ë””ì˜¤ ë¬´ë£Œ ìƒì„± ì˜¤í”ˆì†ŒìŠ¤")
    gr.Markdown("** WAN 2.2 14B + FAST + í•œê¸€í™” + íŠœë‹ ** - 4~8ë‹¨ê³„ë¡œ ë¹ ë¥¸ ì˜ìƒ ìƒì„±")
    gr.Markdown("** íŠ¸ë˜í”½ ì œí•œì‹œ ë‹¤ìŒ 4ê°œì˜ ë¯¸ëŸ¬ë§ ì„œë²„ë“¤ì„ ì´ìš©í•˜ì—¬ ë¶„ì‚° ì‚¬ìš© ê¶Œê³ ")

    gr.HTML(
        """
    <div style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center; margin: 20px 0;">
        <a href="https://huggingface.co/spaces/Heartsync/wan2_2-I2V-14B-FAST" target="_blank">
            <img src="https://img.shields.io/static/v1?label=WAN%202.2%2014B%20FAST%2B&message=Image%20to%20Video&color=%230000ff&labelColor=%23800080&logo=huggingface&logoColor=white&style=for-the-badge" alt="badge">
        </a>
        <a href="https://huggingface.co/spaces/ginipick/wan2_2-I2V-14B-FAST" target="_blank">
            <img src="https://img.shields.io/static/v1?label=WAN%202.2%2014B%20FAST%2B&message=Image%20to%20Video&color=%230000ff&labelColor=%23800080&logo=huggingface&logoColor=white&style=for-the-badge" alt="badge">
        </a>
        <a href="https://huggingface.co/spaces/ginigen/wan2_2-I2V-14B-FAST" target="_blank">
            <img src="https://img.shields.io/static/v1?label=WAN%202.2%2014B%20FAST%2B&message=Image%20to%20Video&color=%230000ff&labelColor=%23800080&logo=huggingface&logoColor=white&style=for-the-badge" alt="badge">
        </a>
        <a href="https://huggingface.co/spaces/VIDraft/wan2_2-I2V-14B-FAST" target="_blank">
            <img src="https://img.shields.io/static/v1?label=WAN%202.2%2014B%20FAST%2B&message=Image%20to%20Video&color=%230000ff&labelColor=%23800080&logo=huggingface&logoColor=white&style=for-the-badge" alt="badge">
        </a>
        <a href="https://discord.gg/openfreeai" target="_blank">
            <img src="https://img.shields.io/static/v1?label=Discord&message=Openfree%20AI&color=%230000ff&labelColor=%23800080&logo=discord&logoColor=white&style=for-the-badge" alt="badge"></a>        
    </div>
    """
    )

    with gr.Row():
        with gr.Column(scale=1):
            input_image_component = gr.Image(type="pil", label="ì…ë ¥ ì´ë¯¸ì§€")
            prompt_input = gr.Textbox(
                label="í”„ë¡¬í”„íŠ¸", value=default_prompt_i2v, lines=2
            )
            duration_seconds_input = gr.Slider(
                minimum=MIN_DURATION,
                maximum=MAX_DURATION,
                step=0.1,
                value=3.5,
                label="ì˜ìƒ ê¸¸ì´ (ì´ˆ)",
            )

            with gr.Accordion("ê³ ê¸‰ ì„¤ì •", open=False):
                negative_prompt_input = gr.Textbox(
                    label="ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸", value=default_negative_prompt, lines=2
                )
                steps_slider = gr.Slider(
                    minimum=1, maximum=30, step=1, value=6, label="ìƒì„± ë‹¨ê³„"
                )
                guidance_scale_input = gr.Slider(
                    minimum=0.0,
                    maximum=10.0,
                    step=0.5,
                    value=1,
                    label="ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ 1",
                )
                guidance_scale_2_input = gr.Slider(
                    minimum=0.0,
                    maximum=10.0,
                    step=0.5,
                    value=1,
                    label="ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ 2",
                )
                seed_input = gr.Slider(
                    label="ì‹œë“œ", minimum=0, maximum=MAX_SEED, step=1, value=42
                )
                randomize_seed_checkbox = gr.Checkbox(
                    label="ëœë¤ ì‹œë“œ ì‚¬ìš©", value=True
                )

            generate_button = gr.Button("ğŸ¥ ì˜ìƒ ìƒì„±", variant="primary", size="lg")

        with gr.Column(scale=1):
            video_output = gr.Video(
                label="ìƒì„±ëœ ì˜ìƒ", autoplay=True, interactive=False
            )

    ui_inputs = [
        input_image_component,
        prompt_input,
        steps_slider,
        negative_prompt_input,
        duration_seconds_input,
        guidance_scale_input,
        guidance_scale_2_input,
        seed_input,
        randomize_seed_checkbox,
    ]
    generate_button.click(
        fn=generate_video, inputs=ui_inputs, outputs=[video_output, seed_input]
    )

    gr.Examples(
        examples=[
            [
                "wan_i2v_input.JPG",
                "POV ì…€ì¹´ ì˜ìƒ, ì„ ê¸€ë¼ìŠ¤ ë‚€ í° ê³ ì–‘ì´ê°€ ì„œí•‘ë³´ë“œì— ì„œì„œ í¸ì•ˆí•œ ë¯¸ì†Œ. ë°°ê²½ì— ì—´ëŒ€ í•´ë³€(ë§‘ì€ ë¬¼, ë…¹ìƒ‰ ì–¸ë•, êµ¬ë¦„ ë‚€ í‘¸ë¥¸ í•˜ëŠ˜). ì„œí•‘ë³´ë“œê°€ ê¸°ìš¸ì–´ì§€ê³  ê³ ì–‘ì´ê°€ ë°”ë‹¤ë¡œ ë–¨ì–´ì§€ë©° ì¹´ë©”ë¼ê°€ ê±°í’ˆê³¼ í–‡ë¹›ê³¼ í•¨ê»˜ ë¬¼ì†ìœ¼ë¡œ ë¹ ì§. ì ê¹ ë¬¼ì†ì—ì„œ ê³ ì–‘ì´ ì–¼êµ´ ë³´ì´ë‹¤ê°€ ë‹¤ì‹œ ìˆ˜ë©´ ìœ„ë¡œ ì˜¬ë¼ì™€ ì…€ì¹´ ì´¬ì˜ ê³„ì†, ì¦ê±°ìš´ ì—¬ë¦„ íœ´ê°€ ë¶„ìœ„ê¸°.",
                4,
            ],
            [
                "wan22_input_2.jpg",
                "ì„¸ë ¨ëœ ë‹¬ íƒì‚¬ ì°¨ëŸ‰ì´ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë¯¸ë„ëŸ¬ì§€ë“¯ ì´ë™í•˜ë©° ë‹¬ ë¨¼ì§€ë¥¼ ì¼ìœ¼í‚´. í° ìš°ì£¼ë³µì„ ì…ì€ ìš°ì£¼ì¸ë“¤ì´ ë‹¬ íŠ¹ìœ ì˜ ë›°ëŠ” ë™ì‘ìœ¼ë¡œ íƒ‘ìŠ¹. ë¨¼ ë°°ê²½ì—ì„œ VTOL ë¹„í–‰ì²´ê°€ ìˆ˜ì§ìœ¼ë¡œ í•˜ê°•í•˜ì—¬ í‘œë©´ì— ì¡°ìš©íˆ ì°©ë¥™. ì¥ë©´ ì „ì²´ì— ê±¸ì³ ì´ˆí˜„ì‹¤ì ì¸ ì˜¤ë¡œë¼ê°€ ë³„ì´ ê°€ë“í•œ í•˜ëŠ˜ì„ ê°€ë¡œì§€ë¥´ë©° ì¶¤ì¶”ê³ , ë…¹ìƒ‰, íŒŒë€ìƒ‰, ë³´ë¼ìƒ‰ ë¹›ì˜ ì»¤íŠ¼ì´ ë‹¬ í’ê²½ì„ ì‹ ë¹„ë¡­ê³  ë§ˆë²• ê°™ì€ ë¹›ìœ¼ë¡œ ê°ìŒˆ.",
                4,
            ],
            [
                "kill_bill.jpeg",
                "ìš°ë§ˆ ì„œë¨¼ì˜ ìºë¦­í„° ë² ì•„íŠ¸ë¦­ìŠ¤ í‚¤ë„ê°€ ì˜í™” ê°™ì€ ì¡°ëª… ì†ì—ì„œ ë‚ ì¹´ë¡œìš´ ì¹´íƒ€ë‚˜ ê²€ì„ ì•ˆì •ì ìœ¼ë¡œ ë“¤ê³  ìˆìŒ. ê°‘ìê¸° ê´‘íƒ ë‚˜ëŠ” ê°•ì² ì´ ë¶€ë“œëŸ¬ì›Œì§€ê³  ì™œê³¡ë˜ê¸° ì‹œì‘í•˜ë©° ê°€ì—´ëœ ê¸ˆì†ì²˜ëŸ¼ êµ¬ì¡°ì  ì™„ì „ì„±ì„ ìƒê¸° ì‹œì‘. ê²€ë‚ ì˜ ì™„ë²½í•œ ëì´ ì²œì²œíˆ íœ˜ì–´ì§€ê³  ëŠ˜ì–´ì§€ë©°, ë…¹ì€ ê°•ì² ì´ ì€ë¹› ë¬¼ì¤„ê¸°ë¡œ ì•„ë˜ë¡œ í˜ëŸ¬ë‚´ë¦¼. ë³€í˜•ì€ ì²˜ìŒì—ëŠ” ë¯¸ë¬˜í•˜ê²Œ ì‹œì‘ë˜ë‹¤ê°€ ê¸ˆì†ì´ ì ì  ë” ìœ ë™ì ì´ ë˜ë©´ì„œ ê°€ì†í™”. ì¹´ë©”ë¼ëŠ” ê·¸ë…€ì˜ ì–¼êµ´ì„ ê³ ì •í•˜ê³  ë‚ ì¹´ë¡œìš´ ëˆˆë¹›ì´ ì ì°¨ ì¢ì•„ì§€ëŠ”ë°, ì¹˜ëª…ì ì¸ ì§‘ì¤‘ì´ ì•„ë‹ˆë¼ ë¬´ê¸°ê°€ ëˆˆì•ì—ì„œ ë…¹ëŠ” ê²ƒì„ ë³´ë©° í˜¼ë€ê³¼ ê²½ì•…. í˜¸í¡ì´ ì•½ê°„ ë¹¨ë¼ì§€ë©° ì´ ë¶ˆê°€ëŠ¥í•œ ë³€í˜•ì„ ëª©ê²©. ë…¹ëŠ” í˜„ìƒì´ ê°•í™”ë˜ê³  ì¹´íƒ€ë‚˜ì˜ ì™„ë²½í•œ í˜•íƒœê°€ ì ì  ì¶”ìƒì ì´ ë˜ë©° ì†ì—ì„œ ìˆ˜ì€ì²˜ëŸ¼ ë–¨ì–´ì§. ë…¹ì€ ë°©ìš¸ì´ ë¶€ë“œëŸ¬ìš´ ê¸ˆì† ì¶©ê²©ìŒê³¼ í•¨ê»˜ ë°”ë‹¥ì— ë–¨ì–´ì§. í‘œì •ì´ ì°¨ë¶„í•œ ì¤€ë¹„ì—ì„œ ë‹¹í˜¹ê°ê³¼ ìš°ë ¤ë¡œ ë°”ë€Œë©° ì „ì„¤ì ì¸ ë³µìˆ˜ì˜ ë„êµ¬ê°€ ì†ì—ì„œ ë¬¸ì ê·¸ëŒ€ë¡œ ì•¡í™”ë˜ì–´ ë¬´ë°©ë¹„ ìƒíƒœê°€ ë¨.",
                6,
            ],
        ],
        inputs=[input_image_component, prompt_input, steps_slider],
        outputs=[video_output, seed_input],
        fn=generate_video,
        cache_examples="lazy",
    )

if __name__ == "__main__":
    demo.queue().launch(share=True)
